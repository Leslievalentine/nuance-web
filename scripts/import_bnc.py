import os
import psycopg2
import re
from glob import glob
import xml.etree.ElementTree as ET

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
BNC_PATH = os.path.join(BASE_DIR, 'data', 'BNC', 'Texts')

DB_CONFIG = {
    "dbname": "nuance_engine_db", "user": "postgres", "password": "5432", 
    "host": "localhost", "options": "-c client_encoding=utf8"
}

# BNC åˆ†ç±»ä»£ç æ˜ å°„è¡¨ (Codes -> Readable Genres)
GENRE_MAP = {
    'WRIDOM1': 'Literature', 
    'WRIDOM2': 'Natural Sci', 
    'WRIDOM3': 'Applied Sci',
    'WRIDOM4': 'Social Sci', 
    'WRIDOM5': 'World Affairs', 
    'WRIDOM6': 'Commerce',
    'WRIDOM7': 'Arts', 
    'WRIDOM8': 'Belief', 
    'WRIDOM9': 'Leisure',
    'ALLTYP3': 'Spoken (Demographic)',
    'ALLTYP4': 'Spoken (Context)'
}

def robust_extract_genre(filepath):
    """
    æš´åŠ›ä¸”é²æ£’çš„åˆ†ç±»æå–ï¼šä¸è§£æ XML æ ‘ç»“æ„ï¼Œç›´æ¥è¯»å–æ–‡ä»¶å¤´ 5KB æ–‡æœ¬ï¼Œ
    æ­£åˆ™æœç´¢åˆ†ç±»ä»£ç ã€‚è¿™æ˜¯å¤„ç† BNC ç»“æ„ä¸ä¸€è‡´æœ€æœ‰æ•ˆçš„æ–¹æ³•ã€‚
    """
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            # åªè¯»å¤´éƒ¨ï¼Œåˆ†ç±»ä¿¡æ¯é€šå¸¸åœ¨å‰ 2000 å­—ç¬¦å†…
            head = f.read(5000)
            
        # 1. ä¼˜å…ˆåŒ¹é…ä¹¦é¢è¯­åŸŸ (WRIDOM)
        # æŸ¥æ‰¾ target="WRIDOMx" æˆ–è€… ç›´æ¥å‡ºç° WRIDOMx
        match = re.search(r'(WRIDOM\d)', head)
        if match:
            code = match.group(1)
            return GENRE_MAP.get(code, 'Written (Misc)')
            
        # 2. åŒ¹é…å£è¯­è¯­åŸŸ (ALLTYP)
        match_spoken = re.search(r'(ALLTYP\d)', head)
        if match_spoken:
            code = match_spoken.group(1)
            return GENRE_MAP.get(code, 'Spoken (Misc)')
            
        return 'Unclassified'
        
    except Exception:
        return 'Unclassified'

def parse_sentences(filepath):
    """
    è§£æå¥å­ï¼šä»ç„¶ä½¿ç”¨ XML è§£æï¼Œä¿è¯å¥å­å®Œæ•´æ€§
    """
    try:
        tree = ET.parse(filepath)
        root = tree.getroot()
        sents = []
        # æŸ¥æ‰¾æ‰€æœ‰ <s> æ ‡ç­¾
        for s in root.findall('.//s'):
            # æå–å…¶ä¸­çš„ <w> (word) å’Œ <c> (punctuation)
            parts = []
            for node in s.iter():
                if node.tag in ('w', 'c', 'mw') and node.text:
                    parts.append(node.text.strip())
            
            if parts:
                text = " ".join(parts)
                # è¿‡æ»¤å¤ªçŸ­çš„ç¢ç‰‡
                if len(parts) > 3:
                    # ç®€å•åˆ†è¯æ•°ç»„ (ç”¨äºç´¢å¼•)
                    words_arr = [w.lower() for w in parts if w.isalnum()]
                    if words_arr:
                        sents.append((text, words_arr))
        return sents
    except:
        return []

def run_import():
    print("ğŸš‘ [Fix Phase] å¼€å§‹ä¿®å¤ BNC æ•°æ®...")
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    # 1. æ¸…ç†æ—§ BNC æ•°æ®
    print("ğŸ§¹ æ­£å†æ¸…é™¤æ—§çš„ BNC é”™è¯¯æ•°æ®...")
    cur.execute("DELETE FROM corpus_sentences WHERE source_corpus = 'BNC'")
    conn.commit()
    print("âœ… æ¸…ç†å®Œæˆã€‚")

    # 2. é‡æ–°å¯¼å…¥
    files = glob(os.path.join(BNC_PATH, '**', '*.xml'), recursive=True)
    print(f"ğŸ“š é‡æ–°æ‰«æ {len(files)} ä¸ªæ–‡ä»¶...")
    
    buffer = []
    total_saved = 0
    
    for i, fpath in enumerate(files):
        fid = os.path.basename(fpath)
        
        # æå–åˆ†ç±» (ä½¿ç”¨æ–°é€»è¾‘)
        real_genre = robust_extract_genre(fpath)
        
        # æå–å¥å­
        sents = parse_sentences(fpath)
        
        for text, words_arr in sents:
            buffer.append((text, words_arr, 'BNC', real_genre, fid))
            
        # æ‰¹é‡å†™å…¥
        if len(buffer) >= 2000:
            args = ','.join(cur.mogrify("(%s,%s,%s,%s,%s)", x).decode('utf-8') for x in buffer)
            cur.execute(f"INSERT INTO corpus_sentences (sentence_text, words_array, source_corpus, original_genre, file_id) VALUES {args}")
            conn.commit()
            total_saved += len(buffer)
            buffer = []
            print(f"\râ³ ä¿®å¤è¿›åº¦: {i}/{len(files)} | å½“å‰: {real_genre.ljust(15)} | å·²å­˜: {total_saved}", end="")

    # å°¾éƒ¨å¤„ç†
    if buffer:
        args = ','.join(cur.mogrify("(%s,%s,%s,%s,%s)", x).decode('utf-8') for x in buffer)
        cur.execute(f"INSERT INTO corpus_sentences (sentence_text, words_array, source_corpus, original_genre, file_id) VALUES {args}")
        conn.commit()

    print(f"\nğŸ‰ BNC æ•°æ®ä¿®å¤å®Œæˆï¼Unclassified æ¯”ä¾‹åº”å¤§å¹…ä¸‹é™ã€‚")
    cur.close(); conn.close()

if __name__ == "__main__":
    run_import()
