import nltk
import psycopg2
from collections import Counter, defaultdict
import re

# NLTK èµ„æº
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')

DB_CONFIG = {
    "dbname": "nuance_engine_db", "user": "postgres", "password": "5432", 
    "host": "localhost", "options": "-c client_encoding=utf8"
}

class NuanceAnalyzer:
    def __init__(self):
        # 1. é»‘åå•è¯­åŸŸ (ä¸ä¸“ä¸š/å™ªéŸ³å¤§)
        self.GENRE_BLACKLIST = {'spam', 'jokes', 'twitter', 'Unclassified'}
        
        # 2. åœç”¨è¯
        self.stopwords = {
            'the','a','an','and','or','but','is','are','was','were','be','been',
            'this','that','it','he','she','they','we','i','you','my','your',
            'in','on','at','to','for','of','with','by'
        }
        
        # 3. åŠ è½½è¯å½¢è¡¨
        self.lemma_map = self._load_lemma_map()
        self.MIN_SENTENCE_THRESHOLD = 5

    def _load_lemma_map(self):
        print("ğŸ§  Loading Lemmatization Map...")
        lemma_db = {}
        try:
            conn = psycopg2.connect(**DB_CONFIG)
            cur = conn.cursor()
            cur.execute("SELECT spelling, exchange FROM words WHERE exchange IS NOT NULL AND exchange != ''")
            for base, exc in cur.fetchall():
                base = base.lower()
                variants = re.findall(r':[a-zA-Z\-]+', exc)
                for v in variants:
                    lemma_db[v[1:].lower()] = base
            conn.close()
            return lemma_db
        except: return {}

    def normalize_word(self, word):
        return self.lemma_map.get(word.lower(), word.lower())

    def analyze(self, target_word, strategy, sentences_data):
        target_lemma = target_word.lower()
        
        # 1. åŒæºè¯­åŸŸé›·è¾¾ (Dual-Source Radar)
        # ç»“æ„: {"BNC": {"Arts": 10}, "MASC": {"blog": 20}}
        register_stats = {"BNC": Counter(), "MASC": Counter()}
        grouped_sents = defaultdict(list) # æŒ‰è¯­åŸŸåˆ†ç»„ä¾‹å¥
        
        for text, words_arr, source, genre in sentences_data:
            # A. å™ªéŸ³æ¸…æ´—
            if genre in self.GENRE_BLACKLIST: continue
            if text.isupper(): continue # è¿‡æ»¤å…¨å¤§å†™æ ‡é¢˜ (LEAVING A LEGACY)
            if len(words_arr) < 4: continue
            
            # B. ç»Ÿè®¡åˆ†å¸ƒ
            # ç¡®ä¿ source åªæœ‰ BNC/MASCï¼Œé˜²æ­¢è„æ•°æ®
            src_key = source if source in ['BNC', 'MASC'] else 'Other'
            register_stats[src_key][genre] += 1
            
            # C. æ”¶é›†ä¾‹å¥ç”¨äºæ·±åº¦åˆ†æ
            grouped_sents[genre].append((text, words_arr))

        # 2. ç­–ç•¥åˆ†æµ
        analysis_result = {}
        
        # è·å– Top 5 æ´»è·ƒè¯­åŸŸ (åˆå¹¶ BNC å’Œ MASC çš„æ‰€æœ‰è¯­åŸŸæŒ‰æ€»æ•°æ’åº)
        all_genres = Counter()
        for src in register_stats:
            all_genres.update(register_stats[src])
        
        top_genres = [g for g, c in all_genres.most_common(5)]
        
        if strategy == 'PATTERN':
            analysis_result = self._engine_a_pattern(target_lemma, top_genres, grouped_sents)
        elif strategy == 'LINEAR':
            analysis_result = self._engine_b_linear(target_lemma, top_genres, grouped_sents)
            
        return {
            "register": {k: dict(v) for k, v in register_stats.items()}, # è½¬ä¸ºæ™®é€šdict
            "analysis": analysis_result
        }

    # ==========================================================
    # ğŸŸ  Engine A: æ„å¼è§£æ (å‡çº§ç‰ˆ: è¯æ€§æ„ŸçŸ¥)
    # ==========================================================
    def _engine_a_pattern(self, target_lemma, genres, grouped_sents):
        patterns_by_genre = {}
        
        for genre in genres:
            sents = grouped_sents[genre]
            if len(sents) < self.MIN_SENTENCE_THRESHOLD: continue
            
            pattern_counter = Counter()
            examples_map = defaultdict(list)
            
            for text, words_arr in sents:
                try:
                    tagged = nltk.pos_tag(words_arr)
                    
                    # å¯»æ‰¾ç›®æ ‡è¯ï¼Œä¸”å¿…é¡»è¿›è¡Œè¯æ€§æ£€æŸ¥
                    indices = [i for i, (w, t) in enumerate(tagged) 
                               if self.normalize_word(w) == target_lemma]
                    
                    for idx in indices:
                        target_tag = tagged[idx][1]
                        
                        # ğŸ”¥ æ ¸å¿ƒä¿®æ­£: æ ¹æ®ç›®æ ‡è¯æ€§åˆ†æµ
                        pat = None
                        if target_tag.startswith('V'): # åŠ¨è¯
                            pat = self._extract_verb_pattern(tagged, idx)
                        elif target_tag.startswith('N'): # åè¯
                            pat = self._extract_noun_pattern(tagged, idx)
                        elif target_tag.startswith('J'): # å½¢å®¹è¯
                            pat = self._extract_adj_pattern(tagged, idx)
                            
                        if pat:
                            pattern_counter[pat] += 1
                            if len(examples_map[pat]) < 3:
                                examples_map[pat].append(text)
                except: continue
            
            # æ•´ç†ç»“æœ
            top_patterns = []
            for pat, count in pattern_counter.most_common(5):
                if count < 2: continue
                top_patterns.append({
                    "template": pat,
                    "count": count,
                    "examples": examples_map[pat]
                })
            
            if top_patterns:
                patterns_by_genre[genre] = top_patterns
                
        return patterns_by_genre

    def _extract_verb_pattern(self, tagged, idx):
        if idx + 1 >= len(tagged): return None
        next_w, next_t = tagged[idx+1]
        
        if next_w == 'that': return "V + that-clause"
        if next_t == 'TO': return "V + to do"
        if next_t == 'IN': return f"V + {next_w} + n."
        # æ’é™¤ä»£è¯ä¸»æ ¼ï¼Œé˜²æ­¢ä»å¥è¯¯åˆ¤ä¸ºå®¾è¯­
        if (next_t.startswith('N') or next_t.startswith('P')) and next_w not in ['i','he','she','we','they']:
            return "V + object (n.)"
        if idx == 0 or tagged[idx-1][0] == ',': return "Discourse Marker"
        return None

    def _extract_noun_pattern(self, tagged, idx):
        # é’ˆå¯¹ terms, way, idea
        if idx + 1 >= len(tagged): return None
        next_w, next_t = tagged[idx+1]
        
        if next_w == 'of': return "N + of + n."  # way of life
        if next_w == 'that': return "N + that-clause" # idea that...
        if next_t == 'TO': return "N + to do" # way to go
        if next_t == 'IN': return f"N + {next_w} + n." # search for...
        return None
        
    def _extract_adj_pattern(self, tagged, idx):
        if idx + 1 >= len(tagged): return None
        next_w, next_t = tagged[idx+1]
        if next_t == 'TO': return "Adj + to do" # happy to see
        if next_t == 'IN': return f"Adj + {next_w} + n." # good at...
        return None

    # ==========================================================
    # ğŸ”µ Engine B: çº¿æ€§æ­é…
    # ==========================================================
    def _engine_b_linear(self, target_lemma, genres, grouped_sents):
        collabs_by_genre = {}
        
        for genre in genres:
            sents = grouped_sents[genre]
            if len(sents) < self.MIN_SENTENCE_THRESHOLD: continue
            
            modifiers = Counter()
            objects = Counter()
            examples_map = defaultdict(list)
            
            for text, words_arr in sents:
                try:
                    tagged = nltk.pos_tag(words_arr)
                    indices = [i for i, (w, t) in enumerate(tagged) 
                               if self.normalize_word(w) == target_lemma]
                    
                    for idx in indices:
                        start, end = max(0, idx-3), min(len(tagged), idx+4)
                        for i in range(start, end):
                            if i == idx: continue
                            w, t = tagged[i]
                            if not w.isalpha() or w in self.stopwords: continue
                            
                            phrase = ""
                            item_type = None
                            
                            if i < idx: # å‰ç½®ä¿®é¥°
                                if t.startswith('J') or t.startswith('R') or t.startswith('V'):
                                    phrase = f"{w} {target_lemma}"
                                    item_type = 'mod'
                            else: # åç½®æ­é…
                                if t.startswith('N') or t.startswith('I'):
                                    phrase = f"{target_lemma} {w}"
                                    item_type = 'obj'
                            
                            if phrase and item_type:
                                if item_type == 'mod': modifiers[phrase] += 1
                                else: objects[phrase] += 1
                                if len(examples_map[phrase]) < 1:
                                    examples_map[phrase].append(text)
                except: continue
                
            res = {}
            top_mod = [{"p": p, "c": c, "ex": examples_map[p][0]} for p, c in modifiers.most_common(6) if c > 1]
            top_obj = [{"p": p, "c": c, "ex": examples_map[p][0]} for p, c in objects.most_common(6) if c > 1]
            
            if top_mod: res["modifiers"] = top_mod
            if top_obj: res["objects"] = top_obj
            if res: collabs_by_genre[genre] = res
            
        return collabs_by_genre
